\section{Related work}

Convolutional Neural Networks (CNNs) are widely used in the computer vision community for a wide variety of tasks; ranging from image classification to 
object detection to semantic segmentation. It is a bottom-up approach on which we applied an input image, stacked layers of convolutions, nonlinearities and sub-sampling.
Encouraged by the success for vision tasks, researchers applied CNNs to text-related problems. The use of CNNs for sentence modeling traces back to (Col-
lobert and Weston, 2008). Collobert adapted 
CNNs for various natural language processing (NLP) problems including part of speech tagging, chunking, named entity recognition and semantic labeling (cite). 
CNNs for NLP work as an analogy between an image and a text representation. Indeed each word is embedded in a vector representation, which is concatenated as a matrix. 
Word embeddings are not intended to hold spatial information. Thus the width of the convolutional filters is usually the same as the dimension of the word features.

We first discuss our choice of architectures. 
If Recurrent Neural Networks (\textit{mostly GRU and LSTM}) are known to perform well on a broad range of tasks for text, recent comparisons have confirmed the advantage of CNNs 
over RNNs when the task at hand is essentially a keyphrase recognition task [1]. Recognition tasks are at the heart of linguistic interests which mostly focus on contrastive analysis.
Moreover, CNNs are static architectures that, according to specific settings, are more robust 
to the vanishing gradient problem and thus can also model long-term dependency in a sentence (Dauphin et al.,  
Wen et al. (2016) and Adel and Schutze (2017)). Due to the convolution operators involved, 
they can be easily parallelized and may be also be easily used by the CPU, which is a practical solution for avoiding the use of GPUs at test time and wider access our tools.

 All previous work converged on a shared assessment: both CNNs and RNNs provide relevant, but different kinds of information for text classification. 
 However, though several works have studied linguistic structures inherent in RNNs, to our knowledge none of them have focused on CNNs. 
 A first line of research has extensively studied the interpretability of word embeddings and their semantic representations (cite). 
 When it comes to deep architectures, Krizhevsky et al. used LSTMs on character level language as a testbed. They demonstrate the existence of 
 long-range dependencies on real word data. Their analysis is based on gate activation statistics and is thus global. On another side, Li et al. 
 provided new visualization tools for recurrent models. They use decoders, t-SNE, and first derivative saliency in order to shed light on how neural models work.
Our perspective differs from their line of research, as we do not intend to explain how CNNs work on textual data, but rather use their features 
to provide complementary information for linguistic analysis.

Although the usage of RNNs is more common, there exist many visualization tools for CNN analysis, 
inspired by the computer vision field. Such work may help us to highlight the linguistic features learned by a 
CNN. Consequently, our method takes inspiration from those works. Visualization models in computer vision mainly consist 
of inverting latent representations in order to spot active regions or features that are relevant to the classification decision.
One can either train a decoder network or used backpropagation on the input instance to highlight its most relevant features. 
While those methods may hold accurate information in their input recovery, they have two main drawbacks, which are twofold: 
i) they are computationally expensive: the first method requires training a model for each latent representation, and the second relies 
on backpropagation for each submitted sentence. ii) they are highly hyperparameter dependent and may require some finetuning given the task at hand.
On the other hand, Deconvolution Networks, proposed by Zeiler et al in ?, is an off-the-shelf method to project a feature map in the input 
space. It consists of inverting each convolutional layer iteratively, back to the input space. The inverse of a discrete convolution is 
computationally challenging. In response, a coarse approximation may be employed which consists of inverting channels and filter weights 
in a convolutional layer and then transposing their kernel matrix. More details of the deconvolution heuristic are provided in section~\ref{sec:method}.
Deconvolution holds several advantages. Firstly it induces minimal computational requirements compared to previous visualization methods. 
Also, it has been used with success for semantic segmentation on images: in ?; Noh et al demonstrate the efficiency of deconvolution networks 
to predict segmentation masks to identify pixel-wise class labels. Thus deconvolution is able to localize meaningful structure in the input space.
