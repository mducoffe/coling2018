%
% File acl2018.tex
%
%% Based on the style files for ACL-2017, with some changes, which were, in turn,
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}
\usepackage{natbib}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

% my_packages
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{color}

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Deconvolution : a deep tool box for linguistic analysis}

\author{
L. Vanni\textsuperscript{1}, V. Elango\textsuperscript{2}, C. Aguilar\textsuperscript{1}, D. Longrée\textsuperscript{3}, D. Mayaffre\textsuperscript{1}, F. Precioso\textsuperscript{2}, M. Ducoffe\textsuperscript{2}\\
  \textsuperscript{1} Univ. Nice Sophia Antipolis - I3S, UMR UNS-CNRS 7271 06900 Sophia Antipolis, France \\
  \{lvanni, mayaffre\}@unice.fr \\
  \textsuperscript{2} Univ. Nice Sophia Antipolis - BCL, UMR UNS-CNRS 7320 - 06357 Nice CEDEX 4, France \\
  \{ducoffe, precioso\}@unice.fr - ecveer@gmail.com \\
  \textsuperscript{3} Univ. Liège - L.A.S.L.A, Bélgique \\
  dominique.longree@uliege.be\\}
\date{}

\begin{document}
\maketitle
\begin{abstract}
  This document contains the instructions for preparing a camera-ready
  manuscript for the proceedings of ACL 2018. The document itself
  conforms to its own specifications, and is therefore an example of
  what your manuscript should look like. These instructions should be
  used for both papers submitted for review and for final versions of
  accepted papers.  Authors are asked to conform to all the directions
  reported in this document.
\end{abstract}

\input{1_intro}

\input{2_related}

\input{3_comparison}

\input{4_motif}

\input{5_conclusion}

%\bibliography{acl2018}
%\bibliographystyle{acl_natbib}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}

In order to make our experiments reproductible, we going to detail here all the hyperparameters used in our architecture. The neural network is written in python with the library Keras (an tensorflow as backend). 

The embedding use a Word2Vec implementation given by the gensim Library. Here we use the SkipGram model with a window size of 10 words and output vectors of 128 values (embedding dimension).

The textual datas are tokenized by a homemade tokensizer (witch work on English, Latin and French). The corpus is splited into 50 length sequence of words (punctuation is keeped) and each word is converted inta an uniq vectore of 128 value.

The first layer of our model takes the text sequence (as word vectors) and apply on it a weight corresponding to our WordToVec values. Those weight are still trainable during the train of the model.

The second layer is the convolution, a Conv2D in Keras with 512 filters of size $3*128$ (filtering three words at time), with a Relu activation method. Then, there is the Maxpooling (MaxPooling2D) 

(The deconvolution model is identical until here. We replace the rest of the classification model (Dense) by a transposed convolution (Conv2DTranspose).)

The last layers of the model are a Dense layers. One hidden layer of 100 neurons with a Relu activation and one final layer of size eaqual to the number of class with a softmax activation.

All experiments in this paper share the same architecture and the same hyperparameters, and are trained with a cross-entropy method (with an Adam optimizer) with 90\% of the dataset for the training data and 10\% for the validation. The all the tests in this paper are done with new data not included in the original dataset.

\end{document}
