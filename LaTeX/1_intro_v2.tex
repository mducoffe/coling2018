\section{Introduction}
As many other fields of data analysis, Natural Language Processing (NLP) has been strongly impacted by the recent advances in Machine Learning, more particularly with the emergence of Deep Learning techniques. These techniques outperform all other state-of-the-art approaches on a wide range of NLP tasks and so they have been quickly and intensively used in industrial systems. Such systems rely on end-to-end training on large amounts of data, making no prior about the linguistic structure and focusing on stastically frequent patterns. Thus, they somehow step away from computational linguistics as they learn implicit linguistic information automatically without aiming at explaining or even exhibiting classic linguistic structures underlying to the decision.

This is the question we raise in this article and that we intend to address by exhibiting classic linguistic patterns which are indeed exploited implictly in deep architectures to lead to higher performances.
Do neural networks make use of co-occurrences and other standard features, considered in traditional Textual Data Analysis (TDA)? 
Do they also rely on complementary linguistic structure, unreachable by the traditional techniques? If so, projecting neural networks 
features back onto the input space would highlight new linguistic structures would lead to improving the analysis of a corpus and a better understanding on where the power of the deep learning techniques comes from.
Our hypothesis is that deep learning is, of course, sensitive to the linguistic units on which the computation of the key statistical sentences are based, but also sensitive to other phenomena than frequency and other complex linguistic 
observables that the TDA has more difficult to take into account - as would be linguistic patterns (Mellet et Longr√©e, 2009).
Our contribution confronts Textual Data Analysis and Convolutional Neural Networks for text analysis. 
We hijack deconvolution network for image analysis to offer to the linguistic community a new point of view for text analysis that we denote deconvolution saliency. Our deconvolution saliency corresponds to the sum over the 
word embedding of the deconvolution projection of a given feature map. Such score provides a heat-map over 
words in a sentence that promotes the patterns impacting for the classification decision.
We confront z-scoring and deconvolution saliency on three languages: English, 
French and Latin. For all our datasets, deconvolution saliency highlights new linguistic observables, unperceivable with z-scoring.


