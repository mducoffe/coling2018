\section{Introduction}

Neural Networks have had a tremendous impact on Natural Language Processing. 
They outperform the performance of many other state-of-the-art systems on a wide range of NLP tasks 
and are fequently used in industrial technology contexts. Such systems rely on end-to-end training on large amounts of data, 
making no prior assumptions about linguistic structure. Thus, they step away from Textual Data Analysis as used in corpus linguistics since 
they learn implicit linguistic information automatically. However, the nature of this information is unavailable to us. 
Do neural networks make use of redundant information as seen in traditional textual data analysis? 
Or, rather, do they rely also on complementary linguistic structure which is invisible in TDA? If so, projecting neural network 
features back onto the input space would highlight new linguistic structures, improving the analysis of a corpus. 
Our hypothesis is that deep learning is sensitive to the linguistic units on which the computation of the 
key statistical sentences is based as well as to phenomena other than frequency and complex linguistic 
observables. TDA has more difficulty taking such elements into account -- such as linguistic patterns (Mellet et Longr√©e, 2009).
Our contribution compares Textual Data Analysis and Convolutional Neural Networks for text analysis. 
We take advantage of deconvolution networks used in image analysis in order to present a new perspective on text analysis to hee linguistic community that 
that we call deconvolution saliency. Our deconvolution saliency corresponds to the sum over the 
word embedding of the deconvolution projection of a given feature map. Such a score provides a heat-map of 
words in a sentence that highlights the patterns relevant for the classification decision.
We confront z-scoring and deconvolution saliency in three languages: English, 


