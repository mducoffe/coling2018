\section{Introduction}
As in many other fields of data analysis, Natural Language Processing (NLP) has been strongly impacted by the recent advances in Machine Learning, more particularly with the emergence of Deep Learning techniques. These techniques outperform all other state-of-the-art approaches on a wide range of NLP tasks and so they have been quickly and intensively used in industrial systems. Such systems rely on end-to-end training on large amounts of data, making no prior assumptions about linguistic structure and focusing on stastically frequent patterns. Thus, they somehow step away from computational linguistics as they learn implicit linguistic information automatically without aiming at explaining or even exhibiting classic linguistic structures underlying the decision.

This is the question we raise in this article and that we intend to address by exhibiting classic linguistic patterns which are indeed exploited implictly in deep architectures to lead to higher performances.
Do neural networks make use of co-occurrences and other standard features, considered in traditional Textual Data Analysis (TDA)? 
Do they also rely on complementary linguistic structure which is invisible to traditional techniques? If so, projecting neural networks 
features back onto the input space would highlight new linguistic structures would lead to improving the analysis of a corpus and a better understanding on where the power of the Deep Learning techniques comes from.
Our hypothesis is that deep learning is sensitive to the linguistic units on which the computation of the key statistical sentences is based as well as to phenomena than other than frequency and complex linguistic 
observables. The TDA has more difficulty taking such elements into account -- such as linguistic linguistic patterns (Mellet et Longr√©e, 2009).
Our contribution confronts Textual Data Analysis and Convolutional Neural Networks for text analysis. 
We take advantage of deconvolution networks for image analysis in order to present a new perspective on text analysis to the linguistic community that we call deconvolution saliency. Our deconvolution saliency corresponds to the sum over the 
word embedding of the deconvolution projection of a given feature map. Such score provides a heat-map of 
words in a sentence that highlights the pattern relevant for the classification decision.
We examine z-scoring and deconvolution saliency on three languages: English, 
French and Latin. For all our datasets, deconvolution saliency highlights new linguistic observables, invisible with z-scoring alone.


